{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import itertools\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure stopwords are downloaded\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0\n",
       "3  Even though I have great interest in Biblical ...      0\n",
       "4  Im a die hard Dads Army fan and nothing will e...      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews = pd.read_csv(\"./IMDB_movie_review_dataset/Train.csv\")\n",
    "imdb_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grew b 1965 watching loving thunderbirds mates...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>put movie dvd player sat coke chips expectatio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people know particular time past like feel nee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>even though great interest biblical movies bor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im die hard dads army fan nothing ever change ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  grew b 1965 watching loving thunderbirds mates...      0\n",
       "1  put movie dvd player sat coke chips expectatio...      0\n",
       "2  people know particular time past like feel nee...      0\n",
       "3  even though great interest biblical movies bor...      0\n",
       "4  im die hard dads army fan nothing ever change ...      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text pre-processing pipeline\n",
    "# 1. Convert text to lowercase\n",
    "# 2. Remove any links or urls from text\n",
    "# 3. Remove any punctuations\n",
    "# 4. Remove stopwords\n",
    "# 5. Correct misspelling [Time consuming]\n",
    "\n",
    "\n",
    "def text_preprocessing_pipeline(corpus):\n",
    "    # Lowercase\n",
    "    corpus[\"text\"] = corpus[\"text\"].str.lower()\n",
    "\n",
    "    # Remove URLs/hyper links\n",
    "    corpus[\"text\"] = corpus[\"text\"].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "\n",
    "    # Remove non-alphanumeric characters\n",
    "    corpus[\"text\"] = corpus[\"text\"].str.replace(\"[^A-Za-z0-9]+\", \" \", regex=True)\n",
    "\n",
    "    # Remove stopwords\n",
    "    corpus[\"text\"] = corpus[\"text\"].apply(\n",
    "        lambda words: \" \".join(\n",
    "            word.lower() for word in words.split() if word not in stopwords\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Optionally apply text correction (Only if you have ample timeðŸ˜‚)\n",
    "    # corpus[\"text\"] = corpus[\"text\"].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "reviews = text_preprocessing_pipeline(imdb_reviews)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grew b 1965 watching loving thunderbirds mates...</td>\n",
       "      <td>0</td>\n",
       "      <td>[grew, b, 1965, watching, loving, thunderbird,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>put movie dvd player sat coke chips expectatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>[put, movie, dvd, player, sat, coke, chip, exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people know particular time past like feel nee...</td>\n",
       "      <td>0</td>\n",
       "      <td>[people, know, particular, time, past, like, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>even though great interest biblical movies bor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[even, though, great, interest, biblical, movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im die hard dads army fan nothing ever change ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[im, die, hard, dad, army, fan, nothing, ever,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  grew b 1965 watching loving thunderbirds mates...      0   \n",
       "1  put movie dvd player sat coke chips expectatio...      0   \n",
       "2  people know particular time past like feel nee...      0   \n",
       "3  even though great interest biblical movies bor...      0   \n",
       "4  im die hard dads army fan nothing ever change ...      1   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0  [grew, b, 1965, watching, loving, thunderbird,...  \n",
       "1  [put, movie, dvd, player, sat, coke, chip, exp...  \n",
       "2  [people, know, particular, time, past, like, f...  \n",
       "3  [even, though, great, interest, biblical, movi...  \n",
       "4  [im, die, hard, dad, army, fan, nothing, ever,...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloads the WordNet lexical database, which is essential for lemmatization.\n",
    "# WordNet provides word relationships and word forms, allowing the lemmatizer to reduce words to their base or root form (lemma).\n",
    "nltk.download(\"wordnet\")\n",
    "# Downloads the Punkt tokenizer, which is used for sentence and word tokenization.\n",
    "# Tokenization is the process of breaking text into smaller units like sentences or words.\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# A tokenizer that splits text into tokens based on whitespace (spaces, tabs, or newlines).\n",
    "# Itâ€™s a simple way to split words without considering punctuation or sentence boundaries.\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "# A lemmatizer that uses WordNet to reduce words to their base or dictionary form (e.g., \"running\" â†’ \"run\", \"better\" â†’ \"good\").\n",
    "# Unlike stemming, lemmatization considers the meaning of the word to produce valid lemmas.\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "\n",
    "reviews[\"lemmatized_tokens\"] = reviews[\"text\"].apply(lemmatize_text)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tokens = list(reviews[\"lemmatized_tokens\"])\n",
    "token_list = list(itertools.chain(*lemmatized_tokens))\n",
    "count_no = collections.Counter(token_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
